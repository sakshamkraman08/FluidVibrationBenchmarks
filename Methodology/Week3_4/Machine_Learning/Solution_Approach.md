# Network Architecture

A feedforward network with layers 2→32→16→1 was implemented using Flux.jl, trained for 100 epochs with the Adam optimizer and learning rate 0.01 [7].
